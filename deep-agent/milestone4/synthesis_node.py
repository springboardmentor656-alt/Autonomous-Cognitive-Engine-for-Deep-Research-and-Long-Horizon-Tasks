from langchain_ollama import ChatOllama

# üî• Use a stronger model than tinyllama
llm = ChatOllama(
    model="llama3",   # change if needed: mistral / phi3
    temperature=0
)

def synthesis_node(state):
    """
    Final synthesis node.
    Combines all research files and generates a complete structured industry report.
    """

    # 1Ô∏è‚É£ Combine all research files
    combined_research = ""

    for filename, content in state["files"].items():
        if filename != "final_report.txt":
            combined_research += f"\n\n---\nSource: {filename}\n{content}\n"

    # 2Ô∏è‚É£ Strong synthesis prompt (forces long structured output)
    prompt = f"""
You are a senior industry analyst.

Using ONLY the research material provided below,
generate a COMPLETE professional industry report.

STRICT REQUIREMENTS:
- Minimum 1200 words
- Use clear structured headings
- Include:
    ‚Ä¢ Executive Summary
    ‚Ä¢ Introduction
    ‚Ä¢ Industry Background
    ‚Ä¢ Key Applications
    ‚Ä¢ Advantages
    ‚Ä¢ Disadvantages
    ‚Ä¢ Challenges
    ‚Ä¢ Practical Solutions
    ‚Ä¢ Future Outlook
    ‚Ä¢ Strong Conclusion
- Maintain a professional analytical tone
- Do NOT summarize
- Expand each section in detail

Research Material:
{combined_research}

Now generate the FULL final report.
"""

    # 3Ô∏è‚É£ Generate final report
    response = llm.invoke(prompt)

    final_report = response.content

    # 4Ô∏è‚É£ Save to virtual file system
    state["files"]["final_report.txt"] = final_report

    # 5Ô∏è‚É£ Also return output for LangSmith visibility
    return {
        "files": state["files"],
        "output": final_report
    }
